{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9549e07c33b740879c5b9c82efe62785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bed67ce6baa4e36a42bf8a68abc2aaf",
              "IPY_MODEL_7e99dd6308a140d2b7a622ed81144925",
              "IPY_MODEL_aff89f65ef724696af865bdece8cd2f8"
            ],
            "layout": "IPY_MODEL_bb19aba9fa4b451ea63614dfddac88dd"
          }
        },
        "2bed67ce6baa4e36a42bf8a68abc2aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_833163e526b2484ebb05cca220f44c29",
            "placeholder": "​",
            "style": "IPY_MODEL_cf0c05869fd949e49f99e6018960c01a",
            "value": "100%"
          }
        },
        "7e99dd6308a140d2b7a622ed81144925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df535d156192425ea96079437d8015a7",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3badc1a2c444819acdd02101f04d357",
            "value": 60
          }
        },
        "aff89f65ef724696af865bdece8cd2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4a1c29638447ff89755ff330d958de",
            "placeholder": "​",
            "style": "IPY_MODEL_24efd1f1a8834e44b918490f37be285e",
            "value": " 60/60 [07:56&lt;00:00,  9.14s/it]"
          }
        },
        "bb19aba9fa4b451ea63614dfddac88dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "833163e526b2484ebb05cca220f44c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf0c05869fd949e49f99e6018960c01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df535d156192425ea96079437d8015a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3badc1a2c444819acdd02101f04d357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f4a1c29638447ff89755ff330d958de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24efd1f1a8834e44b918490f37be285e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Case study 6 - DQN\n",
        "\n",
        "Changes compared to OG work by Guillaume :\n",
        "- `policy` is now an epsilon-greedy policy when taking rollouts (still deterministic when evaluating pairwise comparisons with target weights)\n",
        "- we keep 2 sets of classifiers (original + target). Target weights get smooth updates in the form of exponentially moving average\n",
        "- we keep past transitions in a circular replay buffer\n",
        "\n",
        "These last two changes help with off policiness/exploration and stability."
      ],
      "metadata": {
        "id": "aezy7DakKlKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "EkyB_lcx6Hj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy.integrate as integrate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import deque\n",
        "from random import sample\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VXpiEnMfsXdd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batchSize = 250\n",
        "numberOfActions = 4\n",
        "numberOfEpochs = 10\n",
        "\n",
        "epsilon        = 0.1      #for the epsilon-greedy policy\n",
        "buffer_size    = 2000\n",
        "num_rollouts   = 250\n",
        "num_iterations = 60\n",
        "beta           = 0.95     #soft updates\n",
        "\n",
        "# Some of the following code is based on a PyTorch tutorial in the official PyTorch website:\n",
        "# Below is the definition of the neural networks used for the pair-wize classification of the actions\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(2, 16).to(device)\n",
        "        self.fc2 = nn.Linear(16, 16).to(device)\n",
        "        self.fc3 = nn.Linear(16, 16).to(device)\n",
        "        self.fc4 = nn.Linear(16, 2).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        logits = self.fc4(x)\n",
        "        return logits\n",
        "\n",
        "# We use the cross entropy loss function\n",
        "lossFunction = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the classifiers for each pairs of actions:\n",
        "classifiers = {}\n",
        "target_classifiers = {}\n",
        "\n",
        "for actionIndex1 in range(numberOfActions):\n",
        "  for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "    net = Net().to(device)\n",
        "    input = torch.randn((1, 2), device=device)\n",
        "    out = net(input)\n",
        "\n",
        "    net.zero_grad()\n",
        "    out.backward(torch.randn((1, 2), device=device))\n",
        "\n",
        "    classifiers[(actionIndex1, actionIndex2)] = net\n",
        "    target_classifiers[(actionIndex1, actionIndex2)] = net   #We initialize params and target params at the same values.\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# Each action is represented by an index in a dictionary. Such that each action is accessed via this index\n",
        "actionsDictionary = {}\n",
        "actionsDictionary.update({0 : 0.1})\n",
        "actionsDictionary.update({1 : 0.4})\n",
        "actionsDictionary.update({2 : 0.7})\n",
        "actionsDictionary.update({3 : 1.0})\n",
        "\n",
        "\n",
        "# Below are the constants used in the simulations of the cancer treatement plan\n",
        "a1 = 0.15\n",
        "a2 = 0.1\n",
        "b1 = 1.2\n",
        "b2 = 1.2\n",
        "c0 = -4\n",
        "c1 = 1\n",
        "c2 = 1\n",
        "d1 = 0.5\n",
        "d2 = 0.5\n",
        "\n",
        "\n",
        "# Below is the definition of delta Y as described in the paper. It takes among its arguments the index of an action that corresponds to the amount of chemical given to the patient\n",
        "def deltaY(XState, YState, initialXState, actionIndex):\n",
        "  if(YState <= 0):\n",
        "    return 0\n",
        "\n",
        "  return a1 * np.maximum(XState, initialXState) - b1 * (actionsDictionary[actionIndex] - d1)\n",
        "\n",
        "# Below is the definition of delta X as described in the paper. It takes among its arguments the index of an action that corresponds to the amount of chemical given to the patient\n",
        "def deltaX(XState, YState, initialYState, actionIndex):\n",
        "\n",
        "  return a2 * np.maximum(YState, initialYState) + b2 * (actionsDictionary[actionIndex] - d2)\n",
        "\n",
        "# Below is just adding the deltas to the states, for 1 time step\n",
        "def simulate1Step(XState, YState, initialXState, initialYState, actionIndex):\n",
        "\n",
        "  return (XState + deltaX(XState, YState, initialYState, actionIndex),\n",
        "          YState + deltaY(XState, YState, initialXState, actionIndex))\n",
        "\n",
        "# Below is the function that returns (0) if the patient has died during the present point in time in the simulation. If the patient lives, it returns 1\n",
        "def checkLifeStatus(previousXState, previousYState, presentXState, presentYState):\n",
        "\n",
        "  def XAsAFunctionOfTime(time):\n",
        "    return previousXState + time * (presentXState - previousXState)\n",
        "\n",
        "  def YAsAFunctionOfTime(time):\n",
        "    return previousYState + time * (presentYState - previousYState)\n",
        "\n",
        "  def lambdaAsAFunctionOfTime(time):\n",
        "    return np.exp(c0 + c1 * YAsAFunctionOfTime(time) + c2 * XAsAFunctionOfTime(time))\n",
        "\n",
        "  lambdaIntegral = integrate.quad(lambdaAsAFunctionOfTime, 0, 1)[0]\n",
        "\n",
        "  probabilityOfDeath = 1 - np.exp(-lambdaIntegral)\n",
        "\n",
        "  if(rng.random() < probabilityOfDeath):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "\n",
        "\n",
        "# A return value of (-1) means that action1 is preferable to action2, a return value of (1) means that action2 is preferable to action1\n",
        "# We follow here the treatement plan for (1) patient under different starting actions (actionIndex1) and (actionIndex2)\n",
        "# It does not make sense to take (XState) and (YState) different from (initialXState) and (initialYState) repectively; I just though at first that\n",
        "# I needed code for simulations which would start somewhere in the middle of the treatement plan (somewhere else than at the beginning); but I didn't change the code to make it cleaner yet.\n",
        "def evaluatePreference(XState, YState,\n",
        "                       initialXState, initialYState,\n",
        "                       actionIndex1, actionIndex2,\n",
        "                       timeIndex,\n",
        "                       policy, classifiers):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  #print(\"here0\")\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  # We simulate (1) time step here:\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "\n",
        "  # We check the life status here:\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  # If the patient has died for (1) of the actions, then the following logic gives the pareto dominance relationship:\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  # We store the maximum toxicity here, that tells us about pareto dominance:\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "\n",
        "  # The following are the remaining time steps in the simulation; they follow the same logic are previously described\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    # The remaining action indices are chosen according to the policy in all remaining simulation steps:\n",
        "    actionIndex1 = policy(classifiers, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy(classifiers, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    # We always store the state X and Y for each action indices. (ActionIndex1) and (ActionIndex2) correspond to the initial action taken at the beginning that have to be compared\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  # The following logic describes the pareto dominance relationship when the patient has survived under the 2 choices of initial actions:\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# A return value of (-1) means that policy1 is preferable to policy2, a return value of (1) means that policy2 is preferable to policy1\n",
        "# The code below is almost identical to the code for function (evaluatePreference) above, except that here, at each time steps, actions are taken\n",
        "# from each of the (2) different policies (policy1) and (policy2).\n",
        "def evaluatePreferenceBetween2Policies(XState, YState,\n",
        "                       initialXState, initialYState,\n",
        "                       timeIndex,\n",
        "                       policy1, policy2, classifiers):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  #print(\"here0\")\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  actionIndex1 = policy1(classifiers, XState, YState)\n",
        "  actionIndex2 = policy2(classifiers, XState, YState)\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "\n",
        "  #print(\"here 0.1\")\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "\n",
        "  #print(\"here1\")\n",
        "\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    actionIndex1 = policy1(classifiers, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy2(classifiers, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def randomPolicy(classifiers, XState, YState):\n",
        "\n",
        "  return np.random.randint(numberOfActions)\n",
        "\n",
        "\n",
        "# (possibly) epsilon-greedy policy with the pair-wise classifiers\n",
        "def policy(classifiers, XState, YState, eps_greedy = False):\n",
        "\n",
        "  # We first choose a random index:\n",
        "  randomInitialActionIndex = np.random.randint(numberOfActions)\n",
        "\n",
        "  if np.random.rand() < epsilon and eps_greedy:\n",
        "      return randomInitialActionIndex\n",
        "\n",
        "\n",
        "  bestActionIndex = randomInitialActionIndex\n",
        "\n",
        "  # (actionIndicesToCheck) gives the series of actions to check successively to find the best action\n",
        "  actionIndicesToCheck = [0, 1, 2, 3]\n",
        "  actionIndicesToCheck.remove(bestActionIndex)\n",
        "\n",
        "  # (classifiersAppliedOnTheStateChoices) is a dictionary containing the classifiers applied on the input state (XState, YState)\n",
        "  # The results are going to be used to classify the actions\n",
        "  classifiersAppliedOnTheStateChoices = {}\n",
        "\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      classifier = classifiers[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      # The best action is given by taking the argmax of the classifier applied on the state. A value of (0) means that, for the classifier at hand,\n",
        "      # (actionIndex1) is pareto dominant to (actionIndex2). A value of (1) give the opposite dominance\n",
        "      classifiersAppliedOnTheStateChoices[(actionIndex1, actionIndex2)] = torch.argmax(\n",
        "          classifier(torch.tensor([[XState, YState]], device=device, dtype=torch.float32))\n",
        "      )\n",
        "\n",
        "  for actionIndexToCheck in actionIndicesToCheck:\n",
        "\n",
        "    # (bestActionIndex) is checked against all possibilities of (actionIndexToCheck) in (actionIndicesToCheck)\n",
        "    # The key (bestActionIndex, actionIndexToCheck) might not correspond to a classifier in the classifiers dictionary, because switching the 2 indices would have just given the reversed classifier.\n",
        "    # Since we might not have learned the classifier corresponding to (bestActionIndex, actionIndexToCheck), because of symmetry, we first check we have it in the outer (if) statement\n",
        "    if (bestActionIndex, actionIndexToCheck) in classifiersAppliedOnTheStateChoices:\n",
        "\n",
        "      # The following means that actionIndexToCheck is pareto dominant to the previous bestActionIndex. So we store it, and continue the outer (for) loop for the other actionIndicesToCheck.\n",
        "      # We do this until we have verified all actions and found the dominant one\n",
        "        if(classifiersAppliedOnTheStateChoices[(bestActionIndex, actionIndexToCheck)] == 1):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "    else:\n",
        "      if(classifiersAppliedOnTheStateChoices[(actionIndexToCheck, bestActionIndex)] == 0):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "\n",
        "  return bestActionIndex\n",
        "\n",
        "\n",
        "ReplayBuffer = deque([], maxlen = buffer_size)\n",
        "#when trying to append elements in ReplayBuffer if len(RB) == maxlen, past samples will be thrown out from the other side"
      ],
      "metadata": {
        "id": "V8wrdnfFsThw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN"
      ],
      "metadata": {
        "id": "ebO6ijU-5_s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in tqdm(range(num_iterations)):\n",
        "\n",
        "  # Sample collection by rollouts\n",
        "  rolloutIndex = 0\n",
        "  while(rolloutIndex < num_rollouts):\n",
        "    # The initial states of the patient are taken to be random, as stated in the paper\n",
        "    initialXState = rng.random() * 2\n",
        "    initialYState = rng.random() * 2\n",
        "\n",
        "    actionIndex1 = policy(classifiers, initialXState, initialYState, eps_greedy=True)   #Choose best action w.r.t. epsilon-greedy policy\n",
        "    actionIndex2 = policy(classifiers, initialXState, initialYState, eps_greedy=True)\n",
        "\n",
        "    if actionIndex1 == actionIndex2:\n",
        "      continue #quick sanity check to keep comparing different actions\n",
        "\n",
        "    # Ordering actions to be consistent with the rest of the framework\n",
        "    actionIndex1, actionIndex2 = min(actionIndex1, actionIndex2), max(actionIndex1, actionIndex2)\n",
        "\n",
        "    # We specify (2) actions (actionIndex1) and (actionIndex2) that are to be compared by the present state of the present pair-wise target classifier\n",
        "    preferenceViaParetoDominance = evaluatePreference(initialXState, initialYState,\n",
        "                    initialXState, initialYState,\n",
        "                    actionIndex1, actionIndex2,\n",
        "                    0,\n",
        "                    policy, target_classifiers)\n",
        "\n",
        "    # We only store cases that have a definite pareto dominance for training. A value of (0) returned by (evaluatePreference) means that none of (actionIndex1) or (actionIndex2) is preferable over the other for the present state\n",
        "    if(preferenceViaParetoDominance != 0):\n",
        "\n",
        "      # We naturally use one-hot encoding for training via the cross-entropy loss\n",
        "      onehotEncoding = torch.tensor([0,0], device=device, dtype=torch.float32)\n",
        "      if(preferenceViaParetoDominance == 1):\n",
        "        onehotEncoding[1] = 1\n",
        "      else:\n",
        "        onehotEncoding[0] = 1\n",
        "\n",
        "      ReplayBuffer.append(\n",
        "          ((actionIndex1, actionIndex2), torch.tensor([initialXState, initialYState], device=device), onehotEncoding)\n",
        "      )\n",
        "\n",
        "      rolloutIndex = rolloutIndex + 1\n",
        "\n",
        "  # Training classifiers\n",
        "  Minibatch = sample(ReplayBuffer, batchSize)\n",
        "\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      relevant_samples = list(filter(lambda tup: tup[0] == (actionIndex1, actionIndex2), Minibatch))\n",
        "\n",
        "      if not relevant_samples:\n",
        "          continue\n",
        "\n",
        "      train_dataloader = DataLoader(relevant_samples, batch_size = len(relevant_samples), shuffle = True)\n",
        "\n",
        "      classifierToTrain = classifiers[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      optimizer = optim.SGD(classifierToTrain.parameters(), lr=0.01)\n",
        "\n",
        "      # Training for (1) specific classifier (classifierToTrain):\n",
        "      for _ in range(numberOfEpochs):\n",
        "        for (_, inputState, preference) in train_dataloader:\n",
        "\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            output = classifierToTrain(inputState)\n",
        "\n",
        "            loss = lossFunction(output, preference)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "      #soft target update\n",
        "      target_state_dict = target_classifiers[(actionIndex1, actionIndex2)].state_dict()\n",
        "      policy_state_dict = classifiers[(actionIndex1, actionIndex2)].state_dict()\n",
        "\n",
        "      for key in policy_state_dict:\n",
        "          target_state_dict[key] = beta*target_state_dict[key] + (1-beta)*policy_state_dict[key]\n",
        "          target_classifiers[(actionIndex1, actionIndex2)].load_state_dict(target_state_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "A3LFg4JHTIf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9549e07c33b740879c5b9c82efe62785",
            "2bed67ce6baa4e36a42bf8a68abc2aaf",
            "7e99dd6308a140d2b7a622ed81144925",
            "aff89f65ef724696af865bdece8cd2f8",
            "bb19aba9fa4b451ea63614dfddac88dd",
            "833163e526b2484ebb05cca220f44c29",
            "cf0c05869fd949e49f99e6018960c01a",
            "df535d156192425ea96079437d8015a7",
            "c3badc1a2c444819acdd02101f04d357",
            "6f4a1c29638447ff89755ff330d958de",
            "24efd1f1a8834e44b918490f37be285e"
          ]
        },
        "outputId": "7b53c73c-ba4a-4aba-8ff0-a03669b0a759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/60 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9549e07c33b740879c5b9c82efe62785"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy = 500\n",
        "\n",
        "finalTumorSizesLearnedPolicy = []\n",
        "finalTumorSizesRandomPolicy = []\n",
        "\n",
        "maximumToxicityWithLearnedPolicy = []\n",
        "maximumToxicityWithRandomPolicy = []\n",
        "\n",
        "for rolloutIndex in range(numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy):\n",
        "\n",
        "    initialXState = rng.random() * 2\n",
        "    initialYState = rng.random() * 2\n",
        "\n",
        "    (xStateLearnedPolicy, yStateLearnedPolicy) = (initialXState, initialYState)\n",
        "    (xStateRandomPolicy, yStateRandomPolicy) = (initialXState, initialYState)\n",
        "\n",
        "    maximumToxicityWithLearnedPolicy.append(0)\n",
        "    maximumToxicityWithRandomPolicy.append(0)\n",
        "\n",
        "    for timeStepIndex in range(6):\n",
        "\n",
        "      actionIndexLearnedPolicy = policy(classifiers, xStateLearnedPolicy, yStateLearnedPolicy)\n",
        "      actionIndexRandomPolicy = np.random.randint(4)\n",
        "\n",
        "      # print(\"(initialXState, initialYState): \", (initialXState, initialYState))\n",
        "      # print(\"(xStateLearnedPolicy, yStateLearnedPolicy): \", (xStateLearnedPolicy, yStateLearnedPolicy))\n",
        "      # print(\"(xStateRandomPolicy, yStateRandomPolicy): \", (xStateRandomPolicy, yStateRandomPolicy))\n",
        "\n",
        "      (xStateLearnedPolicy, yStateLearnedPolicy) = simulate1Step(xStateLearnedPolicy, yStateLearnedPolicy, initialXState, initialYState, actionIndexLearnedPolicy)\n",
        "      (xStateRandomPolicy, yStateRandomPolicy) = simulate1Step(xStateRandomPolicy, yStateRandomPolicy, initialXState, initialYState, actionIndexRandomPolicy)\n",
        "\n",
        "      maximumToxicityWithLearnedPolicy[rolloutIndex] = np.maximum(xStateLearnedPolicy, maximumToxicityWithLearnedPolicy[rolloutIndex])\n",
        "      maximumToxicityWithRandomPolicy[rolloutIndex] = np.maximum(xStateRandomPolicy, maximumToxicityWithRandomPolicy[rolloutIndex])\n",
        "\n",
        "    finalTumorSizesLearnedPolicy.append(yStateLearnedPolicy)\n",
        "    finalTumorSizesRandomPolicy.append(yStateRandomPolicy)\n",
        "\n",
        "print(\"Average Final Tumor Sizes for Learned Policy: \", np.mean(np.array(finalTumorSizesLearnedPolicy)))\n",
        "print(\"Average Final Tumor Sizes for Random Policy: \", np.mean(np.array(finalTumorSizesRandomPolicy)))\n",
        "print(\"Average Maximum Toxicity for Learned Policy: \", np.mean(np.array(maximumToxicityWithLearnedPolicy)))\n",
        "print(\"Average Maximum Toxicity for Random Policy: \", np.mean(np.array(maximumToxicityWithRandomPolicy)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okIfnH8nyiZg",
        "outputId": "a8892bc9-ea0d-4256-8750-3e6c57a84416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Final Tumor Sizes for Learned Policy:  4.783999098226286\n",
            "Average Final Tumor Sizes for Random Policy:  1.8526111010850634\n",
            "Average Maximum Toxicity for Learned Policy:  0.6628490298834889\n",
            "Average Maximum Toxicity for Random Policy:  2.4249693134132073\n"
          ]
        }
      ]
    }
  ]
}