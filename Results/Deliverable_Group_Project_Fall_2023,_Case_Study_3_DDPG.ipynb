{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aezy7DakKlKY"
      },
      "source": [
        "Changes compared to OG work by Guillaume :\n",
        "- policy is now an epsilon-greedy policy\n",
        "- we keep 2 sets of actors (original + target)\n",
        "- we keep past transitions in a replay buffer\n",
        "- use reward shaping to create a preference-based reward function\n",
        "- create a Q-network that takes state and policy network outputs (actions) as inputs\n",
        "- use two networks to make updates (target and main network) for both policy (actor) and Q-netword (critic)\n",
        "- introduce a discount factor to calculate critic target outputs\n",
        "- refractored `simulate1Step`\n",
        "- changed how neural nets are initialized\n",
        "\n",
        "These last two changes help with off policiness and stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVyuYk3H6YhG",
        "outputId": "26f1771f-4059-4ed7-fcd0-0fd389440de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.5/1.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uqq ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXpiEnMfsXdd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy.integrate as integrate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import deque\n",
        "from random import sample\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import ipdb  # Use `!pip install -Uqq ipdb`\n",
        "from typing import Tuple\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8wrdnfFsThw"
      },
      "outputs": [],
      "source": [
        "batchSize = 32\n",
        "numberOfActions = 4\n",
        "numberOfEpochs = 4\n",
        "\n",
        "epsilon        = 0.05     #for the epsilon-greedy policy\n",
        "buffer_size    = 5000\n",
        "num_rollouts   = 10 * batchSize\n",
        "num_iterations = 30\n",
        "tau            = 0.005\n",
        "gamma          = 0.99  # discount factor\n",
        "\n",
        "\n",
        "# Below is the definition of the neural networks used for the pair-wize classification of the actions\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, outdim, use_action=False):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.use_action = use_action\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        self.fc4 = nn.Linear(16, outdim)\n",
        "        if use_action:\n",
        "          self.fca = nn.Linear(2, 16)\n",
        "\n",
        "    def init_weights(m):\n",
        "      if isinstance(m, nn.Linear):\n",
        "          torch.nn.init.xavier_uniform(m.weight)\n",
        "          m.bias.data.fill_(0.01)\n",
        "\n",
        "    def forward(self, state, action=None):\n",
        "\n",
        "        x = F.relu(self.fc1(state))\n",
        "        if self.use_action and action is not None:\n",
        "          x = x + F.relu(self.fca(action))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        logits = self.fc4(x)\n",
        "        return logits\n",
        "\n",
        "# We use the cross entropy loss function\n",
        "lossFunction = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the actors (actors) for each pairs of actions:\n",
        "actors = {}\n",
        "target_actors = {}\n",
        "\n",
        "# Initialization of the Q-nets (critics) for each pairs of actions:\n",
        "critics = {}\n",
        "target_critics = {}\n",
        "\n",
        "\n",
        "for actionIndex1 in range(numberOfActions):\n",
        "  for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "    actor_net = Net(outdim=2)\n",
        "    critic_net = Net(outdim=1, use_action=True)\n",
        "    actor_net.init_weights()\n",
        "    critic_net.init_weights()\n",
        "    actors[(actionIndex1, actionIndex2)] = actor_net\n",
        "    target_actors[(actionIndex1, actionIndex2)] = actor_net\n",
        "    critics[(actionIndex1, actionIndex2)] = critic_net\n",
        "    target_critics[(actionIndex1, actionIndex2)] = critic_net\n",
        "\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# Each action is represented by an index in a dictionary. Such that each action is accessed via this index\n",
        "actionsDictionary = {}\n",
        "actionsDictionary.update({0 : 0.1})\n",
        "actionsDictionary.update({1 : 0.4})\n",
        "actionsDictionary.update({2 : 0.7})\n",
        "actionsDictionary.update({3 : 1.0})\n",
        "\n",
        "\n",
        "# Below are the constants used in the simulations of the cancer treatement plan\n",
        "a1 = 0.15\n",
        "a2 = 0.1\n",
        "b1 = 1.2\n",
        "b2 = 1.2\n",
        "c0 = -4\n",
        "c1 = 1\n",
        "c2 = 1\n",
        "d1 = 0.5\n",
        "d2 = 0.5\n",
        "\n",
        "\n",
        "# Below is just adding the deltas to the states, for 1 time step\n",
        "def simulate1Step(\n",
        "    XState, YState, initialXState, initialYState, actionIndex):\n",
        "  # args:\n",
        "  #     actionIndex: index of an action that corresponds to the amount of chemical given to the patient\n",
        "\n",
        "  def deltaX(XState, YState, initialYState, actionIndex):\n",
        "    return a2 * np.maximum(YState, initialYState) + b2 * (actionsDictionary[actionIndex] - d2)\n",
        "\n",
        "  def deltaY(XState, YState, initialXState, actionIndex):\n",
        "    if(YState > 0):\n",
        "      indicatorFunctionResult = 1\n",
        "    else:\n",
        "      indicatorFunctionResult = 0\n",
        "    actionsDictionary[actionIndex]\n",
        "    np.maximum(XState, initialXState)\n",
        "    return (a1 * np.maximum(XState, initialXState) - b1 * (actionsDictionary[actionIndex] - d1)) * indicatorFunctionResult\n",
        "\n",
        "  new_XState = XState + deltaX(XState, YState, initialYState, actionIndex)\n",
        "  new_YState = YState + deltaY(XState, YState, initialXState, actionIndex)\n",
        "  return (new_XState, new_YState)\n",
        "\n",
        "\n",
        "# Below is the function that returns (0) if the patient has died during the present point in time in the simulation. If the patient lives, it returns 1\n",
        "def checkLifeStatus(previousXState, previousYState, presentXState, presentYState):\n",
        "\n",
        "  def XAsAFunctionOfTime(time):\n",
        "    return previousXState + time * (presentXState - previousXState)\n",
        "\n",
        "  def YAsAFunctionOfTime(time):\n",
        "    return previousYState + time * (presentYState - previousYState)\n",
        "\n",
        "  def lambdaAsAFunctionOfTime(time):\n",
        "    return np.exp(c0 + c1 * YAsAFunctionOfTime(time) + c2 * XAsAFunctionOfTime(time))\n",
        "\n",
        "  lambdaIntegral = integrate.quad(lambdaAsAFunctionOfTime, 0, 1)[0]\n",
        "  probabilityOfDeath = 1 - np.exp(-lambdaIntegral)\n",
        "\n",
        "  if(rng.random() < probabilityOfDeath):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "\n",
        "\n",
        "# A return value of (-1) means that action1 is preferable to action2, a return value of (1) means that action2 is preferable to action1\n",
        "# We follow here the treatement plan for (1) patient under different starting actions (actionIndex1) and (actionIndex2)\n",
        "def evaluatePreference(\n",
        "    XState, YState, initialXState, initialYState, actionIndex1, actionIndex2,\n",
        "    policy, actors, timeIndex=0\n",
        "  ):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  # We simulate (1) time step here:\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "\n",
        "  # We check the life status here:\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  # If the patient has died for (1) of the actions, then the following logic gives the pareto dominance relationship:\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  # We store the maximum toxicity here, that tells us about pareto dominance:\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "  # The following are the remaining time steps in the simulation; they follow the same logic are previously described\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    # The remaining action indices are chosen according to the policy in all remaining simulation steps:\n",
        "    actionIndex1 = policy(actors, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy(actors, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    # We always store the state X and Y for each action indices. (ActionIndex1) and (ActionIndex2) correspond to the initial action taken at the beginning that have to be compared\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  # The following logic describes the pareto dominance relationship when the patient has survived under the 2 choices of initial actions:\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# A return value of (-1) means that policy1 is preferable to policy2, a return value of (1) means that policy2 is preferable to policy1\n",
        "# The code below is almost identical to the code for function (evaluatePreference) above, except that here, at each time steps, actions are taken\n",
        "# from each of the (2) different policies (policy1) and (policy2).\n",
        "def evaluatePreferenceBetween2Policies(\n",
        "    XState, YState, initialXState, initialYState,\n",
        "    timeIndex, policy1, policy2, actors\n",
        "  ):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  actionIndex1 = policy1(actors, XState, YState)\n",
        "  actionIndex2 = policy2(actors, XState, YState)\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    actionIndex1 = policy1(actors, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy2(actors, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# We create of dictionary of the pairs of actions' indices to be able to randomly access the actors;\n",
        "def randomPolicy(actors, XState, YState):\n",
        "  return np.random.randint(numberOfActions)\n",
        "\n",
        "\n",
        "# This is the construction of the epsilon-greedy policy with the pair-wise actors\n",
        "def policy(actors, XState, YState, eps_greedy = False):\n",
        "\n",
        "  # We first choose a random index:\n",
        "  randomInitialActionIndex = np.random.randint(numberOfActions)\n",
        "\n",
        "  if np.random.rand() < epsilon and eps_greedy:\n",
        "      return randomInitialActionIndex\n",
        "\n",
        "\n",
        "  bestActionIndex = randomInitialActionIndex\n",
        "\n",
        "  # (actionIndicesToCheck) gives the series of actions to check successively to find the best action\n",
        "  actionIndicesToCheck = [0, 1, 2, 3]\n",
        "  actionIndicesToCheck.remove(bestActionIndex)\n",
        "\n",
        "  # (actorsAppliedOnTheStateChoices) is a dictionary containing the actors applied on the input state (XState, YState)\n",
        "  # The results are going to be used to classify the actions\n",
        "  actorsAppliedOnTheStateChoices = {}\n",
        "\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      actor = actors[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      # The best action is given by taking the argmax of the actor applied on the state. A value of (0) means that, for the actor at hand,\n",
        "      # (actionIndex1) is pareto dominant to (actionIndex2). A value of (1) give the opposite dominance\n",
        "      actorsAppliedOnTheStateChoices[(actionIndex1, actionIndex2)] = torch.argmax(actor(torch.Tensor([[XState, YState]]))).cpu().numpy()\n",
        "\n",
        "  for actionIndexToCheck in actionIndicesToCheck:\n",
        "\n",
        "    # (bestActionIndex) is checked against all possibilities of (actionIndexToCheck) in (actionIndicesToCheck)\n",
        "    # The key (bestActionIndex, actionIndexToCheck) might not correspond to a actor in the actors dictionary, because switching the 2 indices would have just given the reversed actor.\n",
        "    # Since we might not have learned the actor corresponding to (bestActionIndex, actionIndexToCheck), because of symmetry, we first check we have it in the outer (if) statement\n",
        "    if (bestActionIndex, actionIndexToCheck) in actorsAppliedOnTheStateChoices:\n",
        "\n",
        "      # The following means that actionIndexToCheck is pareto dominant to the previous bestActionIndex. So we store it, and continue the outer (for) loop for the other actionIndicesToCheck.\n",
        "      # We do this until we have verified all actions and found the dominant one\n",
        "        if(actorsAppliedOnTheStateChoices[(bestActionIndex, actionIndexToCheck)] == 1):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "    else:\n",
        "      if(actorsAppliedOnTheStateChoices[(actionIndexToCheck, bestActionIndex)] == 0):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "\n",
        "  return bestActionIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXKmKi8ZPSHz"
      },
      "outputs": [],
      "source": [
        "def checkDoneBatch(\n",
        "    currentStates: torch.Tensor,\n",
        "    nextStates: torch.Tensor\n",
        "  ) -> torch.Tensor:\n",
        "  '''Calls checkLifeStatus for each batch element.\n",
        "\n",
        "  Returns: done = 1 if patient death 0 otherwise.\n",
        "  '''\n",
        "  done = []\n",
        "  for currState, nextState in zip(currentStates.tolist(), nextStates.tolist()):\n",
        "    alive = checkLifeStatus(\n",
        "        currState[0], currState[1], nextState[0], nextState[1]\n",
        "    )\n",
        "    done.append(alive != 1)\n",
        "  return torch.tensor(done, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "\n",
        "def label_smooth_binary_actions(\n",
        "    actions: torch.Tensor\n",
        "  ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  '''Create label smooth actions tensors.'''\n",
        "  actions1 = torch.zeros_like(actions)\n",
        "  actions1[:,0] = 0.9\n",
        "  actions1[:,1] = 0.1\n",
        "  actions2 = torch.zeros_like(actions)\n",
        "  actions2[:,0] = 0.1\n",
        "  actions2[:,1] = 0.9\n",
        "  return actions1, actions2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "03f4fba95ca5401b81345cabe437cf80",
            "4ec764c998e8472da04df3976b382acd",
            "34ee695fe9cb486892524195d003c952",
            "7ef4662787c24d3782172a630d5231a0",
            "fac054e88e154dcb9add37de8ac68279",
            "49e92c7dedac4d9b8cb8c5787f42ece7",
            "2e77d119ef6a49c08c11d8f01a4d70fb",
            "bf517e442faf4ff8bb3fed98bfd12a8c",
            "0c738fa8f42249e1baf47da40efa2004",
            "b223d6d5fc864b93bf4f91615277b13d",
            "c501ea88b549450f9b09d88ab2352f74"
          ]
        },
        "id": "A3LFg4JHTIf9",
        "outputId": "247c1bc8-93e7-4c01-a01f-e1a2bfa35029"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03f4fba95ca5401b81345cabe437cf80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ReplayBuffer = {}\n",
        "\n",
        "for _ in tqdm_notebook(range(num_iterations)):\n",
        "\n",
        "  rolloutIndex = 0\n",
        "\n",
        "  while(rolloutIndex < num_rollouts):\n",
        "    # The initial states of the patient are taken to be random, as stated in the paper\n",
        "    initialXState = rng.random() * 2\n",
        "    initialYState = rng.random() * 2\n",
        "\n",
        "    actionIndex1 = policy(actors, initialXState, initialYState, eps_greedy=True)\n",
        "    actionIndex2 = policy(actors, initialXState, initialYState, eps_greedy=True)\n",
        "    if actionIndex1 == actionIndex2:\n",
        "      continue\n",
        "\n",
        "    actionIndex1, actionIndex2 = min(actionIndex1, actionIndex2), max(actionIndex1, actionIndex2)\n",
        "\n",
        "    preferenceViaParetoDominance = evaluatePreference(\n",
        "        initialXState, initialYState,\n",
        "        initialXState, initialYState,\n",
        "        actionIndex1, actionIndex2,\n",
        "        policy, target_actors\n",
        "    )\n",
        "    (xState1, yState1) = simulate1Step(initialXState, initialYState, initialXState, initialYState, actionIndex1)\n",
        "    (xState2, yState2) = simulate1Step(initialXState, initialYState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    if(preferenceViaParetoDominance != 0):\n",
        "\n",
        "      onehotEncoding = torch.tensor([0,0], device=device, dtype=torch.float32)\n",
        "      if(preferenceViaParetoDominance == 1):  # action2 is preferred\n",
        "        onehotEncoding[1] = 1\n",
        "      else:  # action1 is preferred\n",
        "        onehotEncoding[0] = 1\n",
        "\n",
        "      if (actionIndex1, actionIndex2) not in ReplayBuffer:\n",
        "        ReplayBuffer[(actionIndex1, actionIndex2)] = deque([], maxlen = buffer_size)\n",
        "      ReplayBuffer[(actionIndex1, actionIndex2)].append((\n",
        "          torch.tensor([actionIndex1, actionIndex2], dtype=torch.float32),\n",
        "          torch.tensor([initialXState, initialYState], dtype=torch.float32),\n",
        "          torch.tensor([xState1, yState1], dtype=torch.float32),  # action 1 next state\n",
        "          torch.tensor([xState2, yState2], dtype=torch.float32),  # action 2 next state\n",
        "          onehotEncoding  # preferences\n",
        "      ))\n",
        "\n",
        "      rolloutIndex = rolloutIndex + 1\n",
        "\n",
        "  # We train below the actors with the training elements that we found above:\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      if (actionIndex1, actionIndex2) not in ReplayBuffer:\n",
        "          continue\n",
        "\n",
        "      potential_samples = ReplayBuffer[(actionIndex1, actionIndex2)]\n",
        "      if len(potential_samples) < batchSize:\n",
        "          train_dataloader = DataLoader(potential_samples, batch_size = len(potential_samples), shuffle = True)\n",
        "      else:\n",
        "          training_data = sample(potential_samples, batchSize)\n",
        "          train_dataloader = DataLoader(training_data, batch_size = batchSize, shuffle = True)\n",
        "\n",
        "      actorToTrain = actors[(actionIndex1, actionIndex2)]\n",
        "      criticToTrain = critics[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      optimizer = optim.SGD(actorToTrain.parameters(), lr=0.01)\n",
        "\n",
        "      # Training for (1) specific actor (actorToTrain):\n",
        "      for epochNumber in range(numberOfEpochs):\n",
        "        for (actions, inputState, nextState1, nextState2, preferences) in train_dataloader:\n",
        "\n",
        "            with torch.no_grad():\n",
        "              Q_target_network = target_critics[(actionIndex1, actionIndex2)]\n",
        "              mu_target = target_actors[(actionIndex1, actionIndex2)]\n",
        "              Q_target_next1 = Q_target_network(nextState1, mu_target(nextState1))\n",
        "              Q_target_next2 = Q_target_network(nextState2, mu_target(nextState2))\n",
        "              done1 = checkDoneBatch(inputState, nextState1)\n",
        "              done2 = checkDoneBatch(inputState, nextState2)\n",
        "              reward = preferences\n",
        "              Q_target = reward + torch.cat((\n",
        "                  gamma * (1 - done1) * Q_target_next1,\n",
        "                  gamma * (1 - done2) * Q_target_next2\n",
        "              ), dim=-1)\n",
        "\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            mu = actorToTrain\n",
        "            Q_network = criticToTrain\n",
        "            # Update Q-function weights\n",
        "            actions1, actions2 = label_smooth_binary_actions(actions)\n",
        "            Q_pred = torch.cat((\n",
        "                Q_network(inputState, actions1),\n",
        "                Q_network(inputState, actions2)\n",
        "            ), dim=-1)\n",
        "            # Update policy mu weights\n",
        "            Q_adjusted = Q_network(inputState, mu(inputState)) * preferences\n",
        "            # Calculate the loss\n",
        "            critic_loss = F.mse_loss(Q_pred, Q_target.detach())\n",
        "            actor_loss = -torch.sum(Q_adjusted)\n",
        "            loss = critic_loss + actor_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "      # soft target update\n",
        "      actor_target_state_dict = target_actors[(actionIndex1, actionIndex2)].state_dict()\n",
        "      actor_state_dict = actors[(actionIndex1, actionIndex2)].state_dict()\n",
        "      critic_target_state_dict = target_critics[(actionIndex1, actionIndex2)].state_dict()\n",
        "      critic_state_dict = critics[(actionIndex1, actionIndex2)].state_dict()\n",
        "\n",
        "      for key in actor_state_dict:\n",
        "        actor_target_state_dict[key] = actor_target_state_dict[key]*tau + actor_state_dict[key]*(1-tau)\n",
        "        critic_target_state_dict[key] = critic_target_state_dict[key]*tau + critic_state_dict[key]*(1-tau)\n",
        "      target_actors[(actionIndex1, actionIndex2)].load_state_dict(actor_target_state_dict)\n",
        "      target_critics[(actionIndex1, actionIndex2)].load_state_dict(critic_target_state_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okIfnH8nyiZg",
        "outputId": "50e65e2f-495c-46bd-b9a4-8270d2630380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Final Tumor Sizes for Learned Policy:  0.5433815124156679\n",
            "Average Final Tumor Sizes for Random Policy:  1.8473132806689005\n",
            "Average Maximum Toxicity for Learned Policy:  4.16757385082077\n",
            "Average Maximum Toxicity for Random Policy:  2.397687203293951\n"
          ]
        }
      ],
      "source": [
        "numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy = 500\n",
        "\n",
        "finalTumorSizesLearnedPolicy = []\n",
        "finalTumorSizesRandomPolicy = []\n",
        "\n",
        "maximumToxicityWithLearnedPolicy = []\n",
        "maximumToxicityWithRandomPolicy = []\n",
        "\n",
        "for rolloutIndex in range(numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy):\n",
        "\n",
        "    initialXState = rng.random() * 2\n",
        "    initialYState = rng.random() * 2\n",
        "\n",
        "    (xStateLearnedPolicy, yStateLearnedPolicy) = (initialXState, initialYState)\n",
        "    (xStateRandomPolicy, yStateRandomPolicy) = (initialXState, initialYState)\n",
        "\n",
        "    maximumToxicityWithLearnedPolicy.append(0)\n",
        "    maximumToxicityWithRandomPolicy.append(0)\n",
        "\n",
        "    for timeStepIndex in range(6):\n",
        "\n",
        "      actionIndexLearnedPolicy = policy(actors, xStateLearnedPolicy, yStateLearnedPolicy)\n",
        "      actionIndexRandomPolicy = np.random.randint(4)\n",
        "\n",
        "      (xStateLearnedPolicy, yStateLearnedPolicy) = simulate1Step(xStateLearnedPolicy, yStateLearnedPolicy, initialXState, initialYState, actionIndexLearnedPolicy)\n",
        "      (xStateRandomPolicy, yStateRandomPolicy) = simulate1Step(xStateRandomPolicy, yStateRandomPolicy, initialXState, initialYState, actionIndexRandomPolicy)\n",
        "\n",
        "      maximumToxicityWithLearnedPolicy[rolloutIndex] = np.maximum(xStateLearnedPolicy, maximumToxicityWithLearnedPolicy[rolloutIndex])\n",
        "      maximumToxicityWithRandomPolicy[rolloutIndex] = np.maximum(xStateRandomPolicy, maximumToxicityWithRandomPolicy[rolloutIndex])\n",
        "\n",
        "    finalTumorSizesLearnedPolicy.append(yStateLearnedPolicy)\n",
        "    finalTumorSizesRandomPolicy.append(yStateRandomPolicy)\n",
        "\n",
        "print(\"Average Final Tumor Sizes for Learned Policy: \", np.mean(np.array(finalTumorSizesLearnedPolicy)))\n",
        "print(\"Average Final Tumor Sizes for Random Policy: \", np.mean(np.array(finalTumorSizesRandomPolicy)))\n",
        "print(\"Average Maximum Toxicity for Learned Policy: \", np.mean(np.array(maximumToxicityWithLearnedPolicy)))\n",
        "print(\"Average Maximum Toxicity for Random Policy: \", np.mean(np.array(maximumToxicityWithRandomPolicy)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4UHnkHLEh_I",
        "outputId": "10106f26-621f-42d1-87fb-4a1b72037ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Friedman test: Significant differences exist among groups.\n",
            "Critical Difference (CD): 0.7979554672643842\n",
            "Pairwise Differences:\n",
            "Group 1 vs. Group 2: Difference = 2.8000000000000007, Significant\n",
            "Group 2 vs. Group 3: Difference = 6.4, Significant\n",
            "Group 3 vs. Group 4: Difference = 3.5999999999999996, Significant\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata, friedmanchisquare, norm\n",
        "\n",
        "# Example data of three groups with their respective measurements\n",
        "group1 = np.array([10, 12, 15, 14, 16])\n",
        "group2 = np.array([8, 9, 11, 13, 12])\n",
        "group3 = np.array([5, 7, 6, 9, 8])\n",
        "\n",
        "# Combine all data\n",
        "all_data = np.concatenate([group1, group2, group3])\n",
        "\n",
        "# Assign ranks to the combined data\n",
        "ranks = rankdata(all_data)\n",
        "\n",
        "# Reshape ranks according to the original groups\n",
        "ranks_group1 = ranks[:len(group1)]\n",
        "ranks_group2 = ranks[len(group1):len(group1) + len(group2)]\n",
        "ranks_group3 = ranks[len(group1) + len(group2):]\n",
        "\n",
        "# Friedman test to check if there are significant differences among groups\n",
        "statistic, p_value = friedmanchisquare(group1, group2, group3)\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Friedman test: Significant differences exist among groups.\")\n",
        "    # Calculate critical difference\n",
        "    k = 3  # Number of groups\n",
        "    N = len(all_data)  # Total number of observations\n",
        "    CD = norm.ppf(1 - alpha / (2 * k * (k - 1)) ** 0.5) * (k * (k + 1) / (6 * N)) ** 0.5\n",
        "    print(f\"Critical Difference (CD): {CD}\")\n",
        "\n",
        "    # Comparing pairwise differences using the critical difference\n",
        "    pairwise_diff = [(np.abs(np.mean(x) - np.mean(y)), 'Significant' if np.abs(np.mean(x) - np.mean(y)) > CD else 'Not significant')\n",
        "                     for x, y in [(group1, group2), (group1, group3), (group2, group3)]]\n",
        "\n",
        "    print(\"Pairwise Differences:\")\n",
        "    for i, diff in enumerate(pairwise_diff):\n",
        "        print(f\"Group {i + 1} vs. Group {i + 2}: Difference = {diff[0]}, {diff[1]}\")\n",
        "else:\n",
        "    print(\"Friedman test: No significant differences among groups.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpQ5DuavC6L7",
        "outputId": "adff8dd7-43f2-42bd-d3b0-7d1d279c6327"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13.4"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "group1.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO7y1aHZDHWi",
        "outputId": "bfc4729d-2dfc-44df-b174-cb62e4b18217"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10.6"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "group2.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkHC7bZZDJiC",
        "outputId": "52e32d1a-4aa1-40a5-b85b-d4c3cd4d0e2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "group3.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EJykNl9DOII",
        "outputId": "0e4bb9b5-2d90-4b0a-c84a-989b58a0feca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 8. , 10.5, 14. , 13. , 15. ,  4.5,  6.5,  9. , 12. , 10.5,  1. ,\n",
              "        3. ,  2. ,  6.5,  4.5])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKWsqbhwD5xr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03f4fba95ca5401b81345cabe437cf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ec764c998e8472da04df3976b382acd",
              "IPY_MODEL_34ee695fe9cb486892524195d003c952",
              "IPY_MODEL_7ef4662787c24d3782172a630d5231a0"
            ],
            "layout": "IPY_MODEL_fac054e88e154dcb9add37de8ac68279"
          }
        },
        "0c738fa8f42249e1baf47da40efa2004": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e77d119ef6a49c08c11d8f01a4d70fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34ee695fe9cb486892524195d003c952": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf517e442faf4ff8bb3fed98bfd12a8c",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c738fa8f42249e1baf47da40efa2004",
            "value": 2
          }
        },
        "49e92c7dedac4d9b8cb8c5787f42ece7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec764c998e8472da04df3976b382acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49e92c7dedac4d9b8cb8c5787f42ece7",
            "placeholder": "​",
            "style": "IPY_MODEL_2e77d119ef6a49c08c11d8f01a4d70fb",
            "value": "  7%"
          }
        },
        "7ef4662787c24d3782172a630d5231a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b223d6d5fc864b93bf4f91615277b13d",
            "placeholder": "​",
            "style": "IPY_MODEL_c501ea88b549450f9b09d88ab2352f74",
            "value": " 2/30 [00:15&lt;03:39,  7.83s/it]"
          }
        },
        "b223d6d5fc864b93bf4f91615277b13d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf517e442faf4ff8bb3fed98bfd12a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c501ea88b549450f9b09d88ab2352f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fac054e88e154dcb9add37de8ac68279": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}