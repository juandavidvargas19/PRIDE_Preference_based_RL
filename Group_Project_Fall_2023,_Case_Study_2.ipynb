{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy.integrate as integrate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "VXpiEnMfsXdd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 1\n",
        "y = x\n",
        "x = 2\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRnRbHv-TXGA",
        "outputId": "77a4b65b-fc01-4c82-9134-89c647d255c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  2\n",
            "y:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize = 32\n",
        "numberOfActions = 4\n",
        "numberOfEpochs = 4\n",
        "numberOfPolicyValueIterations = 10\n",
        "numberOfElementsRequiredInDataset = 10 * batchSize\n",
        "\n",
        "# Some of the following code is based on a PyTorch tutorial in the official PyTorch website:\n",
        "# Below is the definition of the neural networks used for the pair-wize classification of the actions\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        self.fc4 = nn.Linear(16, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        logits = self.fc4(x)\n",
        "        return logits\n",
        "\n",
        "# We use the cross entropy loss function\n",
        "lossFunction = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the classifiers for each pairs of actions:\n",
        "classifiers = {}\n",
        "\n",
        "for actionIndex1 in range(numberOfActions):\n",
        "  for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "    net = Net()\n",
        "    input = torch.randn(1, 2)\n",
        "    out = net(input)\n",
        "\n",
        "    net.zero_grad()\n",
        "    out.backward(torch.randn(1, 2))\n",
        "\n",
        "    classifiers[(actionIndex1, actionIndex2)] = net\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# Each action is represented by an index in a dictionary. Such that each action is accessed via this index\n",
        "actionsDictionary = {}\n",
        "actionsDictionary.update({0 : 0.1})\n",
        "actionsDictionary.update({1 : 0.4})\n",
        "actionsDictionary.update({2 : 0.7})\n",
        "actionsDictionary.update({3 : 1.0})\n",
        "\n",
        "\n",
        "\n",
        "# Below are the constants used in the simulations of the cancer treatement plan\n",
        "a1 = 0.15\n",
        "a2 = 0.1\n",
        "b1 = 1.2\n",
        "b2 = 1.2\n",
        "c0 = -4\n",
        "c1 = 1\n",
        "c2 = 1\n",
        "d1 = 0.5\n",
        "d2 = 0.5\n",
        "\n",
        "\n",
        "# Below is the definition of delta Y as described in the paper. It takes among its arguments the index of an action that corresponds to the amount of chemical given to the patient\n",
        "def deltaY(XState, YState,\n",
        "           initialXState,\n",
        "           actionIndex):\n",
        "\n",
        "  #print(\"deltaY here -1: \", actionIndex)\n",
        "  if(YState > 0):\n",
        "    indicatorFunctionResult = 1\n",
        "  else:\n",
        "    indicatorFunctionResult = 0\n",
        "\n",
        "  #print(\"deltaY here 0: \", actionIndex)\n",
        "  #print(\"actionsDictionary[actionIndex]: \", actionsDictionary[actionIndex])\n",
        "  actionsDictionary[actionIndex]\n",
        "  np.maximum(XState, initialXState)\n",
        "  return (a1 * np.maximum(XState, initialXState) - b1 * (actionsDictionary[actionIndex] - d1)) * indicatorFunctionResult\n",
        "\n",
        "# Below is the definition of delta X as described in the paper. It takes among its arguments the index of an action that corresponds to the amount of chemical given to the patient\n",
        "def deltaX(XState, YState,\n",
        "           initialYState,\n",
        "           actionIndex):\n",
        "\n",
        "  #print(\"deltaX  here 1: \", actionIndex)\n",
        "\n",
        "  return a2 * np.maximum(YState, initialYState) + b2 * (actionsDictionary[actionIndex] - d2)\n",
        "\n",
        "# Below is just adding the deltas to the states, for 1 time step\n",
        "def simulate1Step(XState, YState,\n",
        "                  initialXState, initialYState,\n",
        "                  actionIndex):\n",
        "\n",
        "  #print(\"here 3\")\n",
        "  return (XState + deltaX(XState, YState, initialYState, actionIndex), YState + deltaY(XState, YState, initialXState, actionIndex))\n",
        "\n",
        "# Below is the function that returns (0) if the patient has died during the present point in time in the simulation. If the patient lives, it returns 1\n",
        "def checkLifeStatus(previousXState, previousYState, presentXState, presentYState):\n",
        "\n",
        "  def XAsAFunctionOfTime(time):\n",
        "    return previousXState + time * (presentXState - previousXState)\n",
        "\n",
        "  def YAsAFunctionOfTime(time):\n",
        "    return previousYState + time * (presentYState - previousYState)\n",
        "\n",
        "  def lambdaAsAFunctionOfTime(time):\n",
        "    return np.exp(c0 + c1 * YAsAFunctionOfTime(time) + c2 * XAsAFunctionOfTime(time))\n",
        "\n",
        "  lambdaIntegral = integrate.quad(lambdaAsAFunctionOfTime, 0, 1)[0]\n",
        "\n",
        "  probabilityOfDeath = 1 - np.exp(-lambdaIntegral)\n",
        "\n",
        "  if(rng.random() < probabilityOfDeath):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "\n",
        "\n",
        "# A return value of (-1) means that action1 is preferable to action2, a return value of (1) means that action2 is preferable to action1\n",
        "# We follow here the treatement plan for (1) patient under different starting actions (actionIndex1) and (actionIndex2)\n",
        "# It does not make sense to take (XState) and (YState) different from (initialXState) and (initialYState) repectively; I just though at first that\n",
        "# I needed code for simulations which would start somewhere in the middle of the treatement plan (somewhere else than at the beginning); but I didn't change the code to make it cleaner yet.\n",
        "def evaluatePreference(XState, YState,\n",
        "                       initialXState, initialYState,\n",
        "                       actionIndex1, actionIndex2,\n",
        "                       timeIndex,\n",
        "                       policy):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  #print(\"here0\")\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  # We simulate (1) time step here:\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "\n",
        "  # We check the life status here:\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  # If the patient has died for (1) of the actions, then the following logic gives the pareto dominance relationship:\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  # We store the maximum toxicity here, that tells us about pareto dominance:\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "\n",
        "  # The following are the remaining time steps in the simulation; they follow the same logic are previously described\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    # The remaining action indices are chosen according to the policy in all remaining simulation steps:\n",
        "    actionIndex1 = policy(classifiers, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy(classifiers, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    # We always store the state X and Y for each action indices. (ActionIndex1) and (ActionIndex2) correspond to the initial action taken at the beginning that have to be compared\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  # The following logic describes the pareto dominance relationship when the patient has survived under the 2 choices of initial actions:\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# A return value of (-1) means that policy1 is preferable to policy2, a return value of (1) means that policy2 is preferable to policy1\n",
        "# The code below is almost identical to the code for function (evaluatePreference) above, except that here, at each time steps, actions are taken\n",
        "# from each of the (2) different policies (policy1) and (policy2).\n",
        "def evaluatePreferenceBetween2Policies(XState, YState,\n",
        "                       initialXState, initialYState,\n",
        "                       timeIndex,\n",
        "                       policy1, policy2, classifiers):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  #print(\"here0\")\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  actionIndex1 = policy1(classifiers, XState, YState)\n",
        "  actionIndex2 = policy2(classifiers, XState, YState)\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "\n",
        "  #print(\"here 0.1\")\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "\n",
        "  #print(\"here1\")\n",
        "\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    actionIndex1 = policy1(classifiers, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy2(classifiers, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "# # We create of dictionary of the pairs of actions' indices to be able to randomly access the classifiers;\n",
        "# # in order to randomly take a pareto optimal action if there are multiple of them.\n",
        "\n",
        "# pairsOfActionsIndices = {}\n",
        "# numberOfPairsOfActions = 0\n",
        "\n",
        "# for actionIndex1 in range(4):\n",
        "#   for actionIndex2 in range(actionIndex1 + 1):\n",
        "\n",
        "#     pairsOfActionsIndices[pairIndex] = (actionIndex1, actionIndex2)\n",
        "\n",
        "#     numberOfPairsOfActions = numberOfPairsOfActions + 1\n",
        "\n",
        "def constantPolicy0(classifiers, XState, YState):\n",
        "  return 0\n",
        "def constantPolicy1(classifiers, XState, YState):\n",
        "  return 1\n",
        "def constantPolicy2(classifiers, XState, YState):\n",
        "  return 2\n",
        "def constantPolicy3(classifiers, XState, YState):\n",
        "  return 3\n",
        "\n",
        "def randomPolicy(classifiers, XState, YState):\n",
        "\n",
        "  return np.random.randint(numberOfActions)\n",
        "\n",
        "# This is the construction of the policy with the pair-wise classifiers\n",
        "def policy(classifiers, XState, YState):\n",
        "\n",
        "  # We first choose a random index:\n",
        "  randomInitialActionIndex = np.random.randint(numberOfActions)\n",
        "  bestActionIndex = randomInitialActionIndex\n",
        "\n",
        "  # (actionIndicesToCheck) gives the series of actions to check successively to find the best action\n",
        "  actionIndicesToCheck = [0, 1, 2, 3]\n",
        "  actionIndicesToCheck.remove(bestActionIndex)\n",
        "\n",
        "  # (classifiersAppliedOnTheStateChoices) is a dictionary containing the classifiers applied on the input state (XState, YState)\n",
        "  # The results are going to be used to classify the actions\n",
        "  classifiersAppliedOnTheStateChoices = {}\n",
        "\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      classifier = classifiers[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      # The best action is given by taking the argmax of the classifier applied on the state. A value of (0) means that, for the classifier at hand,\n",
        "      # (actionIndex1) is pareto dominant to (actionIndex2). A value of (1) give the opposite dominance\n",
        "      classifiersAppliedOnTheStateChoices[(actionIndex1, actionIndex2)] = torch.argmax(classifier(torch.Tensor([[XState, YState]]))).cpu().numpy()\n",
        "\n",
        "  for actionIndexToCheck in actionIndicesToCheck:\n",
        "\n",
        "    # (bestActionIndex) is checked against all possibilities of (actionIndexToCheck) in (actionIndicesToCheck)\n",
        "    # The key (bestActionIndex, actionIndexToCheck) might not correspond to a classifier in the classifiers dictionary, because switching the 2 indices would have just given the reversed classifier.\n",
        "    # Since we might not have learned the classifier corresponding to (bestActionIndex, actionIndexToCheck), because of symmetry, we first check we have it in the outer (if) statement\n",
        "    if (bestActionIndex, actionIndexToCheck) in classifiersAppliedOnTheStateChoices:\n",
        "\n",
        "      # The following means that actionIndexToCheck is pareto dominant to the previous bestActionIndex. So we store it, and continue the outer (for) loop for the other actionIndicesToCheck.\n",
        "      # We do this until we have verified all actions and found the dominant one\n",
        "        if(classifiersAppliedOnTheStateChoices[(bestActionIndex, actionIndexToCheck)] == 1):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "    else:\n",
        "      if(classifiersAppliedOnTheStateChoices[(actionIndexToCheck, bestActionIndex)] == 0):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "\n",
        "  return bestActionIndex"
      ],
      "metadata": {
        "id": "V8wrdnfFsThw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the policy iteration routine:\n",
        "\n",
        "for policyValueIterationIndex in range(numberOfPolicyValueIterations):\n",
        "\n",
        "  # The following dictionary contains the present training data at this specific policy value iteration step\n",
        "  trainingSetDictionary = {}\n",
        "\n",
        "  # Below we collect the training sets for each classifiers:\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      trainingSetDictionary[(actionIndex1, actionIndex2)] = []\n",
        "\n",
        "      rolloutIndex = 0\n",
        "\n",
        "      while(rolloutIndex < numberOfElementsRequiredInDataset):\n",
        "\n",
        "        # The initial states of the patient are taken to be random, as stated in the paper\n",
        "        initialXState = rng.random() * 2\n",
        "        initialYState = rng.random() * 2\n",
        "\n",
        "        # We specify (2) actions (actionIndex1) and (actionIndex2) that are to be compared by the present state of the present pair-wise classifier\n",
        "        preferenceViaParetoDominance = evaluatePreference(initialXState, initialYState,\n",
        "                        initialXState, initialYState,\n",
        "                        actionIndex1, actionIndex2,\n",
        "                        0,\n",
        "                        policy)\n",
        "\n",
        "        # We only store cases that have a definite pareto dominance for training. A value of (0) returned by (evaluatePreference) means that none of (actionIndex1) or (actionIndex2) is preferable over the other for the present state\n",
        "        if(preferenceViaParetoDominance != 0):\n",
        "\n",
        "          # We naturally use one-hot encoding for training via the cross-entropy loss\n",
        "          onehotEncoding = torch.Tensor([0,0])\n",
        "          if(preferenceViaParetoDominance == 1):\n",
        "            onehotEncoding[1] = 1\n",
        "          else:\n",
        "            onehotEncoding[0] = 1\n",
        "\n",
        "          trainingSetDictionary[(actionIndex1, actionIndex2)].append((torch.Tensor([initialXState, initialYState]), onehotEncoding))\n",
        "\n",
        "          rolloutIndex = rolloutIndex + 1\n",
        "\n",
        "  # We train below the classifiers with the training elements that we found above:\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      training_data = trainingSetDictionary[(actionIndex1, actionIndex2)]\n",
        "      train_dataloader = DataLoader(training_data, batch_size = batchSize, shuffle = True)\n",
        "\n",
        "      classifierToTrain = classifiers[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      optimizer = optim.SGD(classifierToTrain.parameters(), lr=0.01)\n",
        "\n",
        "      # Training for (1) specific classifier (classifierToTrain):\n",
        "      for epochNumber in range(numberOfEpochs):\n",
        "        for (inputState, preference) in train_dataloader:\n",
        "\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            output = classifierToTrain(inputState)\n",
        "\n",
        "\n",
        "            loss = lossFunction(output, preference)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "id": "r5tJ0uXesgnf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy = 500\n",
        "\n",
        "# finalTumorSizesLearnedPolicy = []\n",
        "# finalTumorSizesRandomPolicy = []\n",
        "\n",
        "# maximumToxicityWithLearnedPolicy = []\n",
        "# maximumToxicityWithRandomPolicy = []\n",
        "\n",
        "# for rolloutIndex in range(numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy):\n",
        "\n",
        "#   initialXState = rng.random() * 2\n",
        "#   initialYState = rng.random() * 2\n",
        "\n",
        "#   (xStateLearnedPolicy, yStateLearnedPolicy) = (initialXState, initialYState)\n",
        "#   (xStateRandomPolicy, yStateRandomPolicy) = (initialXState, initialYState)\n",
        "\n",
        "#   maximumToxicityWithLearnedPolicy.append(0)\n",
        "#   maximumToxicityWithRandomPolicy.append(0)\n",
        "\n",
        "#   for timeStepIndex in range(6):\n",
        "\n",
        "#     actionIndexLearnedPolicy = policy(classifiers, xStateLearnedPolicy, yStateLearnedPolicy)\n",
        "#     actionIndexRandomPolicy = np.random.randint(4)\n",
        "\n",
        "#     # print(\"(initialXState, initialYState): \", (initialXState, initialYState))\n",
        "#     # print(\"(xStateLearnedPolicy, yStateLearnedPolicy): \", (xStateLearnedPolicy, yStateLearnedPolicy))\n",
        "#     # print(\"(xStateRandomPolicy, yStateRandomPolicy): \", (xStateRandomPolicy, yStateRandomPolicy))\n",
        "\n",
        "#     (xStateLearnedPolicy, yStateLearnedPolicy) = simulate1Step(xStateLearnedPolicy, yStateLearnedPolicy, initialXState, initialYState, actionIndexLearnedPolicy)\n",
        "#     (xStateRandomPolicy, yStateRandomPolicy) = simulate1Step(xStateRandomPolicy, yStateRandomPolicy, initialXState, initialYState, actionIndexRandomPolicy)\n",
        "\n",
        "#     maximumToxicityWithLearnedPolicy[rolloutIndex] = np.maximum(xStateLearnedPolicy, maximumToxicityWithLearnedPolicy[rolloutIndex])\n",
        "#     maximumToxicityWithRandomPolicy[rolloutIndex] = np.maximum(xStateRandomPolicy, maximumToxicityWithRandomPolicy[rolloutIndex])\n",
        "\n",
        "#   finalTumorSizesLearnedPolicy.append(yStateLearnedPolicy)\n",
        "#   finalTumorSizesRandomPolicy.append(yStateRandomPolicy)\n",
        "\n",
        "# print(\"Average Final Tumor Sizes for Learned Policy: \", np.mean(np.array(finalTumorSizesLearnedPolicy)))\n",
        "# print(\"Average Final Tumor Sizes for Random Policy: \", np.mean(np.array(finalTumorSizesRandomPolicy)))\n",
        "\n",
        "# print(\"Average Maximum Toxicity for Learned Policy: \", np.mean(np.array(maximumToxicityWithLearnedPolicy)))\n",
        "# print(\"Average Maximum Toxicity for Random Policy: \", np.mean(np.array(maximumToxicityWithRandomPolicy)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okIfnH8nyiZg",
        "outputId": "50eba1d2-7db0-4f3d-9c19-5aad28e57cbb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Final Tumor Sizes for Learned Policy:  2.8734570180725734\n",
            "Average Final Tumor Sizes for Random Policy:  1.770044668306209\n",
            "Average Maximum Toxicity for Learned Policy:  1.247728429787222\n",
            "Average Maximum Toxicity for Random Policy:  2.3104074160619863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we check if the learned policy is pareto dominant to a random policy where each actions is taken randomly at each steps of the simulation:"
      ],
      "metadata": {
        "id": "Feu-tmIBUs-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paretoDominanceBetweenPoliciesArray = []\n",
        "\n",
        "numberOfValidComparisons = 0\n",
        "\n",
        "for policyEvaluationIndex in range(100000):\n",
        "\n",
        "  initialXState = rng.random() * 2\n",
        "  initialYState = rng.random() * 2\n",
        "\n",
        "  paretoDominanceBetweenPolicies = evaluatePreferenceBetween2Policies(initialXState, initialYState,\n",
        "                       initialXState, initialYState,\n",
        "                       0,\n",
        "                       randomPolicy, policy, classifiers)\n",
        "\n",
        "  paretoDominanceBetweenPoliciesArray.append(paretoDominanceBetweenPolicies)\n",
        "\n",
        "  if(paretoDominanceBetweenPolicies != 0):\n",
        "    numberOfValidComparisons = numberOfValidComparisons + 1\n",
        "\n",
        "print(\"Sum of ParetoDominances: \", np.sum(np.array(paretoDominanceBetweenPoliciesArray)))\n",
        "print(\"Number of Valid Comparisons: \", numberOfValidComparisons)"
      ],
      "metadata": {
        "id": "zOmu3BDN0ljp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98f7862-b858-42bf-cd63-58fa9e197832"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of ParetoDominances:  1722\n",
            "Number of Valid Comparisons:  74542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We numerically check below the statistical significance of the difference between the random policy and the learned policy"
      ],
      "metadata": {
        "id": "hMz7S0vGt78O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sums = []\n",
        "numberOfSumsToComputeTheStandardDeviation = 100\n",
        "\n",
        "\n",
        "for sumIndex in range(numberOfSumsToComputeTheStandardDeviation):\n",
        "  sum = 0\n",
        "\n",
        "  for randomNumberIndex in range(74886):\n",
        "    number = np.random.randint(2)\n",
        "\n",
        "    if(number == 0):\n",
        "      sum = sum + 1\n",
        "    elif(number == 1):\n",
        "      sum = sum - 1\n",
        "\n",
        "  if(sumIndex % 10 == 0):\n",
        "    print(\"sumIndex: \", sumIndex)\n",
        "\n",
        "  sums.append(sum)\n",
        "\n",
        "meanOfSums = 0\n",
        "\n",
        "standardDeviation = np.sqrt(np.mean(np.power(np.array(sums) - meanOfSums, 2)))\n",
        "\n",
        "print(\"standardDeviation: \", standardDeviation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ifp5jrt7F0",
        "outputId": "b62feb41-c701-4431-d594-84859beade04"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumIndex:  0\n",
            "sumIndex:  10\n",
            "sumIndex:  20\n",
            "sumIndex:  30\n",
            "sumIndex:  40\n",
            "sumIndex:  50\n",
            "sumIndex:  60\n",
            "sumIndex:  70\n",
            "sumIndex:  80\n",
            "sumIndex:  90\n",
            "standardDeviation:  272.9952380537067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check below how many standard deviations our sum of pareto dominances is away from the average value of random choices, which is 0. (We are seeing how much time it happens that our learned policy is better than the random policy. If none is better than the other statistically, then the value of our sum of pareto dominances should be near the mean value of a random sum (0), sum of random values of (1) or (-1))"
      ],
      "metadata": {
        "id": "R-_MvoC2z4y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(np.sum(np.array(paretoDominanceBetweenPoliciesArray)) - meanOfSums)/standardDeviation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2qTjxp6z2sS",
        "outputId": "8b70e597-9908-43f1-d68b-6a7ff3669f27"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.30780233485695"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process is clearly well approximated by a Gaussian distribution. And 6 standard deviations or more happens only about twice every 100 000 000 000 000 000 trials (1/(2 * 10^17))\n",
        "\n",
        "This means that our result is statistically significant."
      ],
      "metadata": {
        "id": "SGw9KfgB3eT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compare our learned policy with constant policies, which give a constant amount of chemical to the patients"
      ],
      "metadata": {
        "id": "m7Z69GhM7y_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constantPolicies = []\n",
        "constantPolicies.append(constantPolicy0)\n",
        "constantPolicies.append(constantPolicy1)\n",
        "constantPolicies.append(constantPolicy2)\n",
        "constantPolicies.append(constantPolicy3)\n",
        "\n",
        "paretoDominancesBetweenLearnedAndConstantPoliciesArray = []\n",
        "\n",
        "numberOfValidComparisonsWithConstantPolicies = []\n",
        "\n",
        "for constantPolicyIndex in range(numberOfActions):\n",
        "\n",
        "  numberOfValidComparisonsWithConstantPolicies.append(0)\n",
        "  paretoDominancesBetweenLearnedAndConstantPoliciesArray.append([])\n",
        "\n",
        "  numberOfValidComparisons = 0\n",
        "\n",
        "  for policyEvaluationIndex in range(100000):\n",
        "\n",
        "    initialXState = rng.random() * 2\n",
        "    initialYState = rng.random() * 2\n",
        "\n",
        "    paretoDominanceBetweenPolicies = evaluatePreferenceBetween2Policies(initialXState, initialYState,\n",
        "                        initialXState, initialYState,\n",
        "                        0,\n",
        "                        constantPolicies[constantPolicyIndex], policy, classifiers)\n",
        "\n",
        "    paretoDominancesBetweenLearnedAndConstantPoliciesArray[constantPolicyIndex].append(paretoDominanceBetweenPolicies)\n",
        "\n",
        "    if(paretoDominanceBetweenPolicies != 0):\n",
        "      numberOfValidComparisonsWithConstantPolicies[constantPolicyIndex] = numberOfValidComparisonsWithConstantPolicies[constantPolicyIndex] + 1\n",
        "\n",
        "  print(\"Sum of ParetoDominances for Constant Policy \" + str(constantPolicyIndex) + \": \", np.sum(np.array(paretoDominancesBetweenLearnedAndConstantPoliciesArray[constantPolicyIndex])))\n",
        "  print(\"Number of Valid Comparisons: \", numberOfValidComparisons)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVt3vIP078sx",
        "outputId": "ab471b7d-bc45-4623-c6bd-eac7c3836f40"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of ParetoDominances for Constant Policy 0:  3633\n",
            "Number of Valid Comparisons:  0\n",
            "Sum of ParetoDominances for Constant Policy 1:  -262\n",
            "Number of Valid Comparisons:  0\n",
            "Sum of ParetoDominances for Constant Policy 2:  2625\n",
            "Number of Valid Comparisons:  0\n",
            "Sum of ParetoDominances for Constant Policy 3:  14995\n",
            "Number of Valid Comparisons:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for constantPolicyIndex in range(numberOfActions):\n",
        "  print(\"Sum of ParetoDominances for Constant Policy \" + str(constantPolicyIndex) + \": \", np.sum(np.array(paretoDominancesBetweenLearnedAndConstantPoliciesArray[constantPolicyIndex])))\n",
        "  print(\"Number of Valid Comparisons: \", numberOfValidComparisonsWithConstantPolicies[constantPolicyIndex])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQpSMI0dEeVE",
        "outputId": "cd252f44-0219-479c-9f66-d8356565e6cf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of ParetoDominances for Constant Policy 0:  3633\n",
            "Number of Valid Comparisons:  74425\n",
            "Sum of ParetoDominances for Constant Policy 1:  -262\n",
            "Number of Valid Comparisons:  73318\n",
            "Sum of ParetoDominances for Constant Policy 2:  2625\n",
            "Number of Valid Comparisons:  74127\n",
            "Sum of ParetoDominances for Constant Policy 3:  14995\n",
            "Number of Valid Comparisons:  79347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute below the statistical significance of the results"
      ],
      "metadata": {
        "id": "ytSTfzokFNvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sums = []\n",
        "numberOfSumsToComputeTheStandardDeviation = 100\n",
        "\n",
        "\n",
        "for sumIndex in range(numberOfSumsToComputeTheStandardDeviation):\n",
        "  sum = 0\n",
        "\n",
        "  for randomNumberIndex in range(79347):\n",
        "    number = np.random.randint(2)\n",
        "\n",
        "    if(number == 0):\n",
        "      sum = sum + 1\n",
        "    elif(number == 1):\n",
        "      sum = sum - 1\n",
        "\n",
        "  if(sumIndex % 10 == 0):\n",
        "    print(\"sumIndex: \", sumIndex)\n",
        "\n",
        "  sums.append(sum)\n",
        "\n",
        "meanOfSums = 0\n",
        "\n",
        "standardDeviation = np.sqrt(np.mean(np.power(np.array(sums) - meanOfSums, 2)))\n",
        "\n",
        "print(\"standardDeviation: \", standardDeviation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZLPNqOoFMXO",
        "outputId": "cceff29b-3bec-493b-e2a4-a80bf5ed5eaf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumIndex:  0\n",
            "sumIndex:  10\n",
            "sumIndex:  20\n",
            "sumIndex:  30\n",
            "sumIndex:  40\n",
            "sumIndex:  50\n",
            "sumIndex:  60\n",
            "sumIndex:  70\n",
            "sumIndex:  80\n",
            "sumIndex:  90\n",
            "standardDeviation:  297.7540595860953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(3633/297.8)\n",
        "print(2625/297.8)\n",
        "print(14995/297.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cOVDdozFtAQ",
        "outputId": "7d8b5c8b-a184-432f-b703-fb0f7f8bf307"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.199462726662189\n",
            "8.814640698455339\n",
            "50.35258562793821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that our learned policy is better than giving a low dose level of chemical constantly at a minimum of 12 standard deviations; better than giving a high dose of chemical at a minimum of 8.8 standard deviations; and better than giving an extreme dose of chemical at 50 standard deviations."
      ],
      "metadata": {
        "id": "CIyCDKfsGCZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sums = []\n",
        "numberOfSumsToComputeTheStandardDeviation = 100\n",
        "\n",
        "\n",
        "for sumIndex in range(numberOfSumsToComputeTheStandardDeviation):\n",
        "  sum = 0\n",
        "\n",
        "  for randomNumberIndex in range(73318):\n",
        "    number = np.random.randint(2)\n",
        "\n",
        "    if(number == 0):\n",
        "      sum = sum + 1\n",
        "    elif(number == 1):\n",
        "      sum = sum - 1\n",
        "\n",
        "  if(sumIndex % 10 == 0):\n",
        "    print(\"sumIndex: \", sumIndex)\n",
        "\n",
        "  sums.append(sum)\n",
        "\n",
        "meanOfSums = 0\n",
        "\n",
        "standardDeviation = np.sqrt(np.mean(np.power(np.array(sums) - meanOfSums, 2)))\n",
        "\n",
        "print(\"standardDeviation: \", standardDeviation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHjAjJ7wGySV",
        "outputId": "032df666-8bc5-43a8-898e-161f1f7b6aa8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumIndex:  0\n",
            "sumIndex:  10\n",
            "sumIndex:  20\n",
            "sumIndex:  30\n",
            "sumIndex:  40\n",
            "sumIndex:  50\n",
            "sumIndex:  60\n",
            "sumIndex:  70\n",
            "sumIndex:  80\n",
            "sumIndex:  90\n",
            "standardDeviation:  317.88954056401417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(262/317)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_OmZtLOHcNa",
        "outputId": "67f46885-0f63-4087-fc3c-3ef066ab4669"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8264984227129337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that our learned policy is worse than giving a medium dose of chemical constantly at about 0.82 standard deviations. Such that our learned policy and giving a medium dose of chemical constantly perform about as well"
      ],
      "metadata": {
        "id": "u_4vWMvSFroL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our analysis"
      ],
      "metadata": {
        "id": "-gqkYSNjI0fR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the classifier models below"
      ],
      "metadata": {
        "id": "cKVjz9UvnaDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in classifiers:\n",
        "  torch.save(classifiers[key], \"./Classifier Model for Action Indices \" + str(key[0]) + \" and \" + str(key[1]) + \".pt\")"
      ],
      "metadata": {
        "id": "SwDz9F1onZC-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the classifier models below"
      ],
      "metadata": {
        "id": "oZqsbFX_rWHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {}\n",
        "\n",
        "for actionIndex1 in range(numberOfActions):\n",
        "  for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "    classifiers[(actionIndex1, actionIndex2)] = torch.load(\"./Classifier Model for Action Indices \" + str(actionIndex1) + \" and \" + str(actionIndex2) + \".pt\")"
      ],
      "metadata": {
        "id": "jwagXVf-rVEk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here today (2023-12-18)"
      ],
      "metadata": {
        "id": "vy_pbChrN4bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1742/74886"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsxi2jCzZS2u",
        "outputId": "e8b2eb9b-4a57-4162-84eb-f62cbbc22247"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.023262024944582432"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "\n",
        "for randomNumberIndex in range(74886):\n",
        "  number = np.random.randint(2)\n",
        "\n",
        "  if(number == 0):\n",
        "    sum = sum + 1\n",
        "  elif(number == 1):\n",
        "    sum = sum - 1\n",
        "\n",
        "print(\"sum: \", sum)"
      ],
      "metadata": {
        "id": "YAM8RAf3gtf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a898baae-eb0f-4a26-8b49-bfb5822b0abb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum:  546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "X, y = load_iris(return_X_y=True)"
      ],
      "metadata": {
        "id": "kRHdSgGSf2vP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}