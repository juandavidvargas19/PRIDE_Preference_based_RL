{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy.integrate as integrate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "VXpiEnMfsXdd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 1\n",
        "y = x\n",
        "x = 2\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRnRbHv-TXGA",
        "outputId": "86a1c9fc-2277-4062-aa3d-a645a2a20dd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  2\n",
            "y:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize = 32\n",
        "numberOfActions = 4\n",
        "numberOfEpochs = 4\n",
        "numberOfPolicyValueIterations = 30\n",
        "numberOfElementsRequiredInDataset = 10 * batchSize\n",
        "\n",
        "# Some of the following code is based on a PyTorch tutorial in the official PyTorch website:\n",
        "# Below is the definition of the neural networks used for the pair-wize classification of the actions\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        self.fc4 = nn.Linear(16, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        logits = self.fc4(x)\n",
        "        return logits\n",
        "\n",
        "# We use the cross entropy loss function\n",
        "lossFunction = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the classifiers for each pairs of actions:\n",
        "classifiers = {}\n",
        "\n",
        "for actionIndex1 in range(numberOfActions):\n",
        "  for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "    net = Net()\n",
        "    input = torch.randn(1, 2)\n",
        "    out = net(input)\n",
        "\n",
        "    net.zero_grad()\n",
        "    out.backward(torch.randn(1, 2))\n",
        "\n",
        "    classifiers[(actionIndex1, actionIndex2)] = net\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# Each action is represented by an index in a dictionary. Such that each action is accessed via this index\n",
        "actionsDictionary = {}\n",
        "actionsDictionary.update({0 : 0.1})\n",
        "actionsDictionary.update({1 : 0.4})\n",
        "actionsDictionary.update({2 : 0.7})\n",
        "actionsDictionary.update({3 : 1.0})\n",
        "\n",
        "\n",
        "\n",
        "# Below are the constants used in the simulations of the cancer treatement plan\n",
        "a1 = 0.15\n",
        "a2 = 0.1\n",
        "b1 = 1.2\n",
        "b2 = 1.2\n",
        "c0 = -4\n",
        "c1 = 1\n",
        "c2 = 1\n",
        "d1 = 0.5\n",
        "d2 = 0.5\n",
        "\n",
        "\n",
        "# Below is the definition of delta Y as described in the paper. It takes among its arguments the index of an action that corresponds to the amount of chemical given to the patient\n",
        "def deltaY(XState, YState,\n",
        "           initialXState,\n",
        "           actionIndex):\n",
        "\n",
        "  #print(\"deltaY here -1: \", actionIndex)\n",
        "  if(YState > 0):\n",
        "    indicatorFunctionResult = 1\n",
        "  else:\n",
        "    indicatorFunctionResult = 0\n",
        "\n",
        "  #print(\"deltaY here 0: \", actionIndex)\n",
        "  #print(\"actionsDictionary[actionIndex]: \", actionsDictionary[actionIndex])\n",
        "  actionsDictionary[actionIndex]\n",
        "  np.maximum(XState, initialXState)\n",
        "  return (a1 * np.maximum(XState, initialXState) - b1 * (actionsDictionary[actionIndex] - d1)) * indicatorFunctionResult\n",
        "\n",
        "# Below is the definition of delta X as described in the paper. It takes among its arguments the index of an action that corresponds to the amount of chemical given to the patient\n",
        "def deltaX(XState, YState,\n",
        "           initialYState,\n",
        "           actionIndex):\n",
        "\n",
        "  #print(\"deltaX  here 1: \", actionIndex)\n",
        "\n",
        "  return a2 * np.maximum(YState, initialYState) + b2 * (actionsDictionary[actionIndex] - d2)\n",
        "\n",
        "# Below is just adding the deltas to the states, for 1 time step\n",
        "def simulate1Step(XState, YState,\n",
        "                  initialXState, initialYState,\n",
        "                  actionIndex):\n",
        "\n",
        "  #print(\"here 3\")\n",
        "  return (XState + deltaX(XState, YState, initialYState, actionIndex), YState + deltaY(XState, YState, initialXState, actionIndex))\n",
        "\n",
        "# Below is the function that returns (0) if the patient has died during the present point in time in the simulation. If the patient lives, it returns 1\n",
        "def checkLifeStatus(previousXState, previousYState, presentXState, presentYState):\n",
        "\n",
        "  def XAsAFunctionOfTime(time):\n",
        "    return previousXState + time * (presentXState - previousXState)\n",
        "\n",
        "  def YAsAFunctionOfTime(time):\n",
        "    return previousYState + time * (presentYState - previousYState)\n",
        "\n",
        "  def lambdaAsAFunctionOfTime(time):\n",
        "    return np.exp(c0 + c1 * YAsAFunctionOfTime(time) + c2 * XAsAFunctionOfTime(time))\n",
        "\n",
        "  lambdaIntegral = integrate.quad(lambdaAsAFunctionOfTime, 0, 1)[0]\n",
        "\n",
        "  probabilityOfDeath = 1 - np.exp(-lambdaIntegral)\n",
        "\n",
        "  if(rng.random() < probabilityOfDeath):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "\n",
        "\n",
        "# A return value of (-1) means that action1 is preferable to action2, a return value of (1) means that action2 is preferable to action1\n",
        "# We follow here the treatement plan for (1) patient under different starting actions (actionIndex1) and (actionIndex2)\n",
        "# It does not make sense to take (XState) and (YState) different from (initialXState) and (initialYState) repectively; I just though at first that\n",
        "# I needed code for simulations which would start somewhere in the middle of the treatement plan (somewhere else than at the beginning); but I didn't change the code to make it cleaner yet.\n",
        "def evaluatePreference(XState, YState,\n",
        "                       initialXState, initialYState,\n",
        "                       actionIndex1, actionIndex2,\n",
        "                       timeIndex,\n",
        "                       policy):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  #print(\"here0\")\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  # We simulate (1) time step here:\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "\n",
        "  # We check the life status here:\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  # If the patient has died for (1) of the actions, then the following logic gives the pareto dominance relationship:\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  # We store the maximum toxicity here, that tells us about pareto dominance:\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "\n",
        "  # The following are the remaining time steps in the simulation; they follow the same logic are previously described\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    # The remaining action indices are chosen according to the policy in all remaining simulation steps:\n",
        "    actionIndex1 = policy(classifiers, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy(classifiers, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    # We always store the state X and Y for each action indices. (ActionIndex1) and (ActionIndex2) correspond to the initial action taken at the beginning that have to be compared\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  # The following logic describes the pareto dominance relationship when the patient has survived under the 2 choices of initial actions:\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# A return value of (-1) means that policy1 is preferable to policy2, a return value of (1) means that policy2 is preferable to policy1\n",
        "# The code below is almost identical to the code for function (evaluatePreference) above, except that here, at each time steps, actions are taken\n",
        "# from each of the (2) different policies (policy1) and (policy2).\n",
        "def evaluatePreferenceBetween2Policies(XState, YState,\n",
        "                       initialXState, initialYState,\n",
        "                       timeIndex,\n",
        "                       policy1, policy2, classifiers):\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = XState\n",
        "  maximumToxicityWithActionIndex2 = XState\n",
        "\n",
        "  #print(\"here0\")\n",
        "\n",
        "  previousXStateWithActionIndex1 = XState\n",
        "  previousXStateWithActionIndex2 = XState\n",
        "\n",
        "  previousYStateWithActionIndex1 = YState\n",
        "  previousYStateWithActionIndex2 = YState\n",
        "\n",
        "  actionIndex1 = policy1(classifiers, XState, YState)\n",
        "  actionIndex2 = policy2(classifiers, XState, YState)\n",
        "\n",
        "  (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex1)\n",
        "\n",
        "  #print(\"here 0.1\")\n",
        "  (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XState, YState, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "\n",
        "  #print(\"here1\")\n",
        "\n",
        "  lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "  lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "  if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "    return 1\n",
        "  elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "    return -1\n",
        "  elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "    return 0\n",
        "\n",
        "  maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "  maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  timeIndex = timeIndex + 1\n",
        "\n",
        "  while(timeIndex < 6):\n",
        "\n",
        "    actionIndex1 = policy1(classifiers, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    actionIndex2 = policy2(classifiers, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    previousXStateWithActionIndex1 = XStateWithActionIndex1\n",
        "    previousXStateWithActionIndex2 = XStateWithActionIndex2\n",
        "\n",
        "    previousYStateWithActionIndex1 = YStateWithActionIndex1\n",
        "    previousYStateWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "    (XStateWithActionIndex1, YStateWithActionIndex1) = simulate1Step(XStateWithActionIndex1, YStateWithActionIndex1, initialXState, initialYState, actionIndex1)\n",
        "    (XStateWithActionIndex2, YStateWithActionIndex2) = simulate1Step(XStateWithActionIndex2, YStateWithActionIndex2, initialXState, initialYState, actionIndex2)\n",
        "\n",
        "    lifeStatusWithActionIndex1 = checkLifeStatus(previousXStateWithActionIndex1, previousYStateWithActionIndex1, XStateWithActionIndex1, YStateWithActionIndex1)\n",
        "    lifeStatusWithActionIndex2 = checkLifeStatus(previousXStateWithActionIndex2, previousYStateWithActionIndex2, XStateWithActionIndex2, YStateWithActionIndex2)\n",
        "\n",
        "    if(lifeStatusWithActionIndex2 > lifeStatusWithActionIndex1):\n",
        "      return 1\n",
        "    elif(lifeStatusWithActionIndex2 < lifeStatusWithActionIndex1):\n",
        "      return -1\n",
        "    elif((lifeStatusWithActionIndex2 == 0) and (lifeStatusWithActionIndex1 == 0)):\n",
        "      return 0\n",
        "\n",
        "    maximumToxicityWithActionIndex1 = np.maximum(maximumToxicityWithActionIndex1, XStateWithActionIndex1)\n",
        "    maximumToxicityWithActionIndex2 = np.maximum(maximumToxicityWithActionIndex2, XStateWithActionIndex2)\n",
        "\n",
        "    timeIndex = timeIndex + 1\n",
        "\n",
        "  tumorSizeAtTheEndWithActionIndex1 = YStateWithActionIndex1\n",
        "  tumorSizeAtTheEndWithActionIndex2 = YStateWithActionIndex2\n",
        "\n",
        "  if((tumorSizeAtTheEndWithActionIndex2 < tumorSizeAtTheEndWithActionIndex1) and (maximumToxicityWithActionIndex2 < maximumToxicityWithActionIndex1)):\n",
        "    return 1\n",
        "  elif((tumorSizeAtTheEndWithActionIndex1 < tumorSizeAtTheEndWithActionIndex2) and (maximumToxicityWithActionIndex1 < maximumToxicityWithActionIndex2)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "# # We create of dictionary of the pairs of actions' indices to be able to randomly access the classifiers;\n",
        "# # in order to randomly take a pareto optimal action if there are multiple of them.\n",
        "\n",
        "# pairsOfActionsIndices = {}\n",
        "# numberOfPairsOfActions = 0\n",
        "\n",
        "# for actionIndex1 in range(4):\n",
        "#   for actionIndex2 in range(actionIndex1 + 1):\n",
        "\n",
        "#     pairsOfActionsIndices[pairIndex] = (actionIndex1, actionIndex2)\n",
        "\n",
        "#     numberOfPairsOfActions = numberOfPairsOfActions + 1\n",
        "\n",
        "def randomPolicy(classifiers, XState, YState):\n",
        "\n",
        "  return np.random.randint(numberOfActions)\n",
        "\n",
        "# This is the construction of the policy with the pair-wise classifiers\n",
        "def policy(classifiers, XState, YState):\n",
        "\n",
        "  # We first choose a random index:\n",
        "  randomInitialActionIndex = np.random.randint(numberOfActions)\n",
        "  bestActionIndex = randomInitialActionIndex\n",
        "\n",
        "  # (actionIndicesToCheck) gives the series of actions to check successively to find the best action\n",
        "  actionIndicesToCheck = [0, 1, 2, 3]\n",
        "  actionIndicesToCheck.remove(bestActionIndex)\n",
        "\n",
        "  # (classifiersAppliedOnTheStateChoices) is a dictionary containing the classifiers applied on the input state (XState, YState)\n",
        "  # The results are going to be used to classify the actions\n",
        "  classifiersAppliedOnTheStateChoices = {}\n",
        "\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      classifier = classifiers[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      # The best action is given by taking the argmax of the classifier applied on the state. A value of (0) means that, for the classifier at hand,\n",
        "      # (actionIndex1) is pareto dominant to (actionIndex2). A value of (1) give the opposite dominance\n",
        "      classifiersAppliedOnTheStateChoices[(actionIndex1, actionIndex2)] = torch.argmax(classifier(torch.Tensor([[XState, YState]]))).cpu().numpy()\n",
        "\n",
        "  for actionIndexToCheck in actionIndicesToCheck:\n",
        "\n",
        "    # (bestActionIndex) is checked against all possibilities of (actionIndexToCheck) in (actionIndicesToCheck)\n",
        "    # The key (bestActionIndex, actionIndexToCheck) might not correspond to a classifier in the classifiers dictionary, because switching the 2 indices would have just given the reversed classifier.\n",
        "    # Since we might not have learned the classifier corresponding to (bestActionIndex, actionIndexToCheck), because of symmetry, we first check we have it in the outer (if) statement\n",
        "    if (bestActionIndex, actionIndexToCheck) in classifiersAppliedOnTheStateChoices:\n",
        "\n",
        "      # The following means that actionIndexToCheck is pareto dominant to the previous bestActionIndex. So we store it, and continue the outer (for) loop for the other actionIndicesToCheck.\n",
        "      # We do this until we have verified all actions and found the dominant one\n",
        "        if(classifiersAppliedOnTheStateChoices[(bestActionIndex, actionIndexToCheck)] == 1):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "    else:\n",
        "      if(classifiersAppliedOnTheStateChoices[(actionIndexToCheck, bestActionIndex)] == 0):\n",
        "\n",
        "          bestActionIndex = actionIndexToCheck\n",
        "\n",
        "  return bestActionIndex"
      ],
      "metadata": {
        "id": "V8wrdnfFsThw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the policy iteration loop, where the policy is learned"
      ],
      "metadata": {
        "id": "ifT9uzIlKbkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the policy iteration routine:\n",
        "# Jonathan: this is the big policy iteration loop! :\n",
        "\n",
        "for policyValueIterationIndex in range(numberOfPolicyValueIterations):\n",
        "\n",
        "  # The following dictionary contains the present training data at this specific policy value iteration step\n",
        "  trainingSetDictionary = {}\n",
        "\n",
        "  # Below we collect the training sets for each classifiers:\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      trainingSetDictionary[(actionIndex1, actionIndex2)] = []\n",
        "\n",
        "      rolloutIndex = 0\n",
        "\n",
        "      while(rolloutIndex < numberOfElementsRequiredInDataset):\n",
        "\n",
        "        # The initial states of the patient are taken to be random, as stated in the paper\n",
        "        initialXState = rng.random() * 2\n",
        "        initialYState = rng.random() * 2\n",
        "\n",
        "        # We specify (2) actions (actionIndex1) and (actionIndex2) that are to be compared by the present state of the present pair-wise classifier\n",
        "        preferenceViaParetoDominance = evaluatePreference(initialXState, initialYState,\n",
        "                        initialXState, initialYState,\n",
        "                        actionIndex1, actionIndex2,\n",
        "                        0,\n",
        "                        policy)\n",
        "\n",
        "        # We only store cases that have a definite pareto dominance for training. A value of (0) returned by (evaluatePreference) means that none of (actionIndex1) or (actionIndex2) is preferable over the other for the present state\n",
        "        if(preferenceViaParetoDominance != 0):\n",
        "\n",
        "          # We naturally use one-hot encoding for training via the cross-entropy loss\n",
        "          onehotEncoding = torch.Tensor([0,0])\n",
        "          if(preferenceViaParetoDominance == 1):\n",
        "            onehotEncoding[1] = 1\n",
        "          else:\n",
        "            onehotEncoding[0] = 1\n",
        "\n",
        "          trainingSetDictionary[(actionIndex1, actionIndex2)].append((torch.Tensor([initialXState, initialYState]), onehotEncoding))\n",
        "\n",
        "          rolloutIndex = rolloutIndex + 1\n",
        "\n",
        "  # We train below the classifiers with the training elements that we found above:\n",
        "  for actionIndex1 in range(numberOfActions):\n",
        "    for actionIndex2 in range(actionIndex1 + 1, numberOfActions):\n",
        "\n",
        "      training_data = trainingSetDictionary[(actionIndex1, actionIndex2)]\n",
        "      train_dataloader = DataLoader(training_data, batch_size = batchSize, shuffle = True)\n",
        "\n",
        "      classifierToTrain = classifiers[(actionIndex1, actionIndex2)]\n",
        "\n",
        "      optimizer = optim.SGD(classifierToTrain.parameters(), lr=0.01)\n",
        "\n",
        "      # Training for (1) specific classifier (classifierToTrain):\n",
        "      for epochNumber in range(numberOfEpochs):\n",
        "        for (inputState, preference) in train_dataloader:\n",
        "\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            output = classifierToTrain(inputState)\n",
        "\n",
        "\n",
        "            loss = lossFunction(output, preference)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "id": "r5tJ0uXesgnf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy = 500\n",
        "\n",
        "# finalTumorSizesLearnedPolicy = []\n",
        "# finalTumorSizesRandomPolicy = []\n",
        "\n",
        "# maximumToxicityWithLearnedPolicy = []\n",
        "# maximumToxicityWithRandomPolicy = []\n",
        "\n",
        "# for rolloutIndex in range(numberOfRolloutsToTestIfLearnedPolicyIsBetterThanRandomPolicy):\n",
        "\n",
        "#   initialXState = rng.random() * 2\n",
        "#   initialYState = rng.random() * 2\n",
        "\n",
        "#   (xStateLearnedPolicy, yStateLearnedPolicy) = (initialXState, initialYState)\n",
        "#   (xStateRandomPolicy, yStateRandomPolicy) = (initialXState, initialYState)\n",
        "\n",
        "#   maximumToxicityWithLearnedPolicy.append(0)\n",
        "#   maximumToxicityWithRandomPolicy.append(0)\n",
        "\n",
        "#   for timeStepIndex in range(6):\n",
        "\n",
        "#     actionIndexLearnedPolicy = policy(classifiers, xStateLearnedPolicy, yStateLearnedPolicy)\n",
        "#     actionIndexRandomPolicy = np.random.randint(4)\n",
        "\n",
        "#     # print(\"(initialXState, initialYState): \", (initialXState, initialYState))\n",
        "#     # print(\"(xStateLearnedPolicy, yStateLearnedPolicy): \", (xStateLearnedPolicy, yStateLearnedPolicy))\n",
        "#     # print(\"(xStateRandomPolicy, yStateRandomPolicy): \", (xStateRandomPolicy, yStateRandomPolicy))\n",
        "\n",
        "#     (xStateLearnedPolicy, yStateLearnedPolicy) = simulate1Step(xStateLearnedPolicy, yStateLearnedPolicy, initialXState, initialYState, actionIndexLearnedPolicy)\n",
        "#     (xStateRandomPolicy, yStateRandomPolicy) = simulate1Step(xStateRandomPolicy, yStateRandomPolicy, initialXState, initialYState, actionIndexRandomPolicy)\n",
        "\n",
        "#     maximumToxicityWithLearnedPolicy[rolloutIndex] = np.maximum(xStateLearnedPolicy, maximumToxicityWithLearnedPolicy[rolloutIndex])\n",
        "#     maximumToxicityWithRandomPolicy[rolloutIndex] = np.maximum(xStateRandomPolicy, maximumToxicityWithRandomPolicy[rolloutIndex])\n",
        "\n",
        "#   finalTumorSizesLearnedPolicy.append(yStateLearnedPolicy)\n",
        "#   finalTumorSizesRandomPolicy.append(yStateRandomPolicy)\n",
        "\n",
        "# print(\"Average Final Tumor Sizes for Learned Policy: \", np.mean(np.array(finalTumorSizesLearnedPolicy)))\n",
        "# print(\"Average Final Tumor Sizes for Random Policy: \", np.mean(np.array(finalTumorSizesRandomPolicy)))\n",
        "\n",
        "# print(\"Average Maximum Toxicity for Learned Policy: \", np.mean(np.array(maximumToxicityWithLearnedPolicy)))\n",
        "# print(\"Average Maximum Toxicity for Random Policy: \", np.mean(np.array(maximumToxicityWithRandomPolicy)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okIfnH8nyiZg",
        "outputId": "50eba1d2-7db0-4f3d-9c19-5aad28e57cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Final Tumor Sizes for Learned Policy:  2.8734570180725734\n",
            "Average Final Tumor Sizes for Random Policy:  1.770044668306209\n",
            "Average Maximum Toxicity for Learned Policy:  1.247728429787222\n",
            "Average Maximum Toxicity for Random Policy:  2.3104074160619863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we check if the learned policy is pareto dominant to a random policy where each actions is taken randomly at each steps of the simulation:"
      ],
      "metadata": {
        "id": "Feu-tmIBUs-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paretoDominanceBetweenPoliciesArray = []\n",
        "\n",
        "numberOfValidComparisons = 0\n",
        "\n",
        "for policyEvaluationIndex in range(300000):\n",
        "\n",
        "  initialXState = rng.random() * 2\n",
        "  initialYState = rng.random() * 2\n",
        "\n",
        "  paretoDominanceBetweenPolicies = evaluatePreferenceBetween2Policies(initialXState, initialYState,\n",
        "                       initialXState, initialYState,\n",
        "                       0,\n",
        "                       randomPolicy, policy, classifiers)\n",
        "\n",
        "  paretoDominanceBetweenPoliciesArray.append(paretoDominanceBetweenPolicies)\n",
        "\n",
        "  if(paretoDominanceBetweenPolicies != 0):\n",
        "    numberOfValidComparisons = numberOfValidComparisons + 1\n",
        "\n",
        "print(\"Sum of ParetoDominances: \", np.sum(np.array(paretoDominanceBetweenPoliciesArray)))\n",
        "print(\"Number of Valid Comparisons: \", numberOfValidComparisons)"
      ],
      "metadata": {
        "id": "zOmu3BDN0ljp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1742/74886"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsxi2jCzZS2u",
        "outputId": "e8b2eb9b-4a57-4162-84eb-f62cbbc22247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.023262024944582432"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "\n",
        "for randomNumberIndex in range(74886):\n",
        "  number = np.random.randint(2)\n",
        "\n",
        "  if(number == 0):\n",
        "    sum = sum + 1\n",
        "  elif(number == 1):\n",
        "    sum = sum - 1\n",
        "\n",
        "print(\"sum: \", sum)"
      ],
      "metadata": {
        "id": "YAM8RAf3gtf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e5f463c-d8e6-46a2-aa92-c1b742568aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum:  -54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRHdSgGSf2vP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}